{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46c17ce-3295-4cbe-9b2a-afbea309e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d83e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ClosedStack_data_Virtual.csv', low_memory= False, encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Job ID\",\"Job Status\",\"Job Klass\",\"Job Hardware ID\",\"Job Operation\",\"Job Created\",\"Job Updated\",\n",
    "#\"Job Duration in Seconds\",\"Job Ticket ID\",\"Hardware ID\",\"Hardware Name\",\"Hardware Image ID\",\n",
    "#\"Hardware Solution ID\",\"Hardware Site ID\",\"Image ID\",\"Image Name\",\"Site ID\",\"Site Name\",\"Ticket Site Name\",\n",
    "#\"First Site Name\",\"Solution ID\",\"Solution Name\",\"Ticket ID\",\"Ticket Site\",\"Hardware Compute Config ID\",\n",
    "#\"Compute Config ID\",\"Compute Config Name\",\"Compute Config CPU\",\"Compute Config Memory\",\"Compute Config Disk\",\n",
    "#\"Compute Config SKU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ticket site, site name, solution name, hardware name/id, job created (in years/months)\n",
    "#df_year_2021 = df_main.loc[(df_main['Job Created'] <= \"2022-01-01 00:00:00.000000\") & (df_main['Job Created'] >= \"2021-01-01 00:00:00.000000\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b4d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[[ 'Job ID','Hardware ID', 'Hardware Name', 'Solution ID', 'Ticket Site','Site Name', 'Job Created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d426470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yearly_data = {}\n",
    "\n",
    "start_year = 2018\n",
    "end_year = 2022\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    monthly_data = {}\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        monthstart = f\"{year}-{month:02d}-01\"\n",
    "        monthend = f\"{year}-{month + 1:02d}-01\" if month < 12 else f\"{year + 1}-01-01\"\n",
    "        \n",
    "        filtered_df = df_new.loc[(df_new['Job Created'] >= monthstart) & (df_new['Job Created'] < monthend)]\n",
    "        monthly_data[f'df_new_{year}_{month:02d}'] = filtered_df\n",
    "    \n",
    "    \n",
    "    yearly_data[year] = monthly_data\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        variable_name = f'df{year}{month}'\n",
    "        length = len(yearly_data[year][f'df_new_{year}_{month}'])\n",
    "        exec(f\"{variable_name} = {length}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c1a5a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839\n"
     ]
    }
   ],
   "source": [
    "print (df202201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7a7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801\n"
     ]
    }
   ],
   "source": [
    "print (df202101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numberofjobspermonth2018 = [df201801, df201802, df201803, df201804, df201805, df201806, df201807, df201808, df201809, df201810, df201811, df201812]\n",
    "yearly_index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number of Jobs' : numberofjobspermonth2018}, index=yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per month, 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofjobspermonth2019 = [df201901, df201902, df201903, df201904, df201905, df201906, df201907, df201908, df201909, df201910, df201911, df201912]\n",
    "yearly_index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number of Jobs' : numberofjobspermonth2019}, index=yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per month, 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofjobspermonth2020 = [df202001, df202002, df202003, df202004, df202005, df202006, df202007, df202008, df202009, df202010, df202011, df202012]\n",
    "yearly_index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number of Jobs' : numberofjobspermonth2020}, index=yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per month, 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofjobspermonth2021 = [df202101, df202102, df202103, df202104, df202105, df202106, df202107, df202108, df202109, df202110, df202111, df202112]\n",
    "yearly_index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number of Jobs' : numberofjobspermonth2021}, index=yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per month, 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofjobspermonth2022 = [df202201, df202202, df202203, df202204, df202205, df202206, df202207, df202208, df202209, df202210, df202211, df202212]\n",
    "yearly_index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number of Jobs' : numberofjobspermonth2022}, index=yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per month, 2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cbdbf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df202202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e012af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = df[[ 'Job ID','Hardware ID', 'Hardware Name', 'Solution Name', 'Ticket Site',' First Site Name', 'Job Created']]\n",
    "# df_new\n",
    "df_new2019 = df_new.loc[(df_new['Job Created'] <= \"2020-01-01 00:00:00.000000\") & (df_new['Job Created'] >= \"2019-01-01 00:00:00.000000\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_new2019.dropna(subset=['Job ID','Job Created', 'Solution Name'])\n",
    "catagorical_cols = ['Job ID', 'Solution Name']\n",
    "#Job Id, Job Created, Ticket Site, Solution Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf5a4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "X_categorical = OneHotEncoder().fit_transform(df_filtered[catagorical_cols].values).toarray()\n",
    "#make new set in x (issue area/ all inputs) then transform into 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d70e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alaina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "df_filtered = df_new2019.dropna(subset=['Job ID', 'Job Created', 'Solution Name'])\n",
    "#create new dataframe the removes rows with missing values for issuearea and direction\n",
    "\n",
    "X = df_filtered[['Job ID', 'Solution Name']].values\n",
    "#defines input and reshaped into array with one column and as many rows as there are data points\n",
    "y = df_filtered['Job Created']\n",
    "#defines output\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_categorical, y, test_size=0.3, random_state=42)\n",
    "# splits input and output variables into 2 sets each, testing set is 30% of the total data, random state ensures reproductibility\n",
    "#train is part of the data added that is used to compare to the test\n",
    "#hasnt see part of the data so it isn't 100% correct (data is split off only for training)\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "# Create an MLPClassifier\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "# Train the classifier on the input and output variables of the training set\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "# Get predictions on the output variable using the trained classifier and the input variable\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute the confusion matrix and accuracy of the predictions compared to true values of output variable.\n",
    "#confusion matrix gives the number of true positive, true negative, falso pos and flase negative\n",
    "#accuracy is the proportion of correct predictions out of all predictions\n",
    "\n",
    "# Print the confusion matrix and accuracy, diagonal should be bigger than top right and bottom left\n",
    "print(cm)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25cf3c88",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['Solution ID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3188\\3642724752.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_new2019\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Job ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Job Created'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Solution ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#create new dataframe the removes rows with missing values for issuearea and direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcatagorical_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Job ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Solution ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alaina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6417\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6418\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6419\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6420\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6421\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6422\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ['Solution ID']"
     ]
    }
   ],
   "source": [
    "df_filtered = df_new2019.dropna(subset=['Job ID', 'Job Created', 'Solution ID'])\n",
    "#create new dataframe the removes rows with missing values for issuearea and direction\n",
    "catagorical_cols = ['Job ID', 'Solution ID']\n",
    "\n",
    "X = df_filtered[['Job ID', 'Solution ID']].values\n",
    "#defines input and reshaped into array with one column and as many rows as there are data points\n",
    "y = df_filtered['Job Created']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce08bb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['Solution ID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3188\\351225968.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Load your dataset into a pandas DataFrame (replace 'your_dataset.csv' with your actual dataset file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdf_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_new2019\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Job ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Job Created'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Solution ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Preprocess your data and create X and y (features and labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alaina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6417\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6418\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6419\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6420\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6421\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6422\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ['Solution ID']"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_dataset.csv' with your actual dataset file)\n",
    "df_filtered = df_new2019.dropna(subset=['Job ID', 'Job Created', 'Solution ID'])\n",
    "\n",
    "\n",
    "# Preprocess your data and create X and y (features and labels)\n",
    "# For example:\n",
    "# X = df[['Job ID', 'Solution Name']].values\n",
    "# y = df['Job Created']\n",
    "pca = PCA(n_components=50)  # You can adjust the number of components based on your needs\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# Encode categorical variables\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X_pca).toarray()\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define custom neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_encoded.shape[1], 128)  # Input size is the number of features after encoding\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output size is 1 for regression task\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    mse = criterion(test_outputs, y_test_tensor)\n",
    "    print(\"Mean Squared Error on Test Data:\", mse.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f533ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fb9e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2018= df_new2018['Job ID'].unique()\n",
    "jobsper2018 = len(jobs2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a51f43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2019= df_new2019['Job ID'].unique()\n",
    "jobsper2019 = len(jobs2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25d0230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2020= df_new2020['Job ID'].unique()\n",
    "jobsper2020 = len(jobs2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cf07e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2021= df_new2021['Job ID'].unique()\n",
    "jobsper2021 = len(jobs2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "171905db",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2022= df_new2022['Job ID'].unique()\n",
    "jobsper2022 = len(jobs2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofjobsperyear = [jobsper2018, jobsper2019, jobsper2020, jobsper2021, jobsper2022]\n",
    "yearly_index = ['2018', '2019', '2020', '2021', '2022']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number of Jobs' : numberofjobsperyear}, index=yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff5c8ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CTC-KC-III', 'STCK-25-DC01', 'CTC-LS-IV', 'CTC-KC-I', 'CTC-LS-VI',\n",
       "       'SLOUGHEQUINIXLD5-DC1', 'CTC-LS-III', 'CTC-KC-II', 'CTC-KC-VIII',\n",
       "       'MARK-371-FL01', 'CTC-LS-V', 'CTC-KC-VII', nan, 'WGC-20-FL01',\n",
       "       'CTC-LS-II', 'CTC-KCBUILD1', 'BRISISEEKEAGLE-DC1', 'CLIENT-SITE',\n",
       "       'UPPLANDS-DS1-DC02', 'BRMP-30-FL01', 'Unanderra-IT3',\n",
       "       'PAN-110-DC01', 'CTC-LS-IV ', 'CTC-LS-I', 'KALR-39-FL04',\n",
       "       'MLVN-35-DC01', 'BRISISEEKWLNGABA-DC1', 'BRLN-50-DC01'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df['Ticket Site Name'].unique()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "006c4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticketsite1 = df[(df['Ticket Site Name']=='CTC-KC-III')]\n",
    "df_ticketsite2 = df[(df['Ticket Site Name']=='STCK-25-DC01')]\n",
    "df_ticketsite3 = df[(df['Ticket Site Name']=='CTC-LS-IV')]\n",
    "df_ticketsite4 = df[(df['Ticket Site Name']=='CTC-KC-I')]\n",
    "df_ticketsite5 = df[(df['Ticket Site Name']=='CTC-LS-VI')]\n",
    "df_ticketsite6 = df[(df['Ticket Site Name']=='SLOUGHEQUINIXLD5-DC1')]\n",
    "df_ticketsite7 = df[(df['Ticket Site Name']=='CTC-LS-III')]\n",
    "df_ticketsite8 = df[(df['Ticket Site Name']=='CTC-KC-II')]\n",
    "df_ticketsite9 = df[(df['Ticket Site Name']=='CTC-KC-VIII')]\n",
    "df_ticketsite10 = df[(df['Ticket Site Name']=='MARK-371-FL01')]\n",
    "ts1 = len(df_ticketsite1)\n",
    "ts2 = len(df_ticketsite2)\n",
    "ts3 = len(df_ticketsite3)\n",
    "ts4 = len(df_ticketsite4)\n",
    "ts5 = len(df_ticketsite5)\n",
    "ts6 = len(df_ticketsite6)\n",
    "ts7 = len(df_ticketsite7)\n",
    "ts8 = len(df_ticketsite8)\n",
    "ts9 = len(df_ticketsite9)\n",
    "ts10 = len(df_ticketsite10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa159cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofjobsatgivendcen = [ts1, ts2, ts3, ts4, ts5, ts6, ts7, ts8, ts9, ts10]\n",
    "yearly_index = ['CTC-KC-III', 'STCK-25-DC01', 'CTC-LS-IV', 'CTC-KC-I', 'CTC-LS-VI',\n",
    "       'SLOUGHEQUINIXLD5-DC1', 'CTC-LS-III', 'CTC-KC-II', 'CTC-KC-VIII',\n",
    "       'MARK-371-FL01']\n",
    "\n",
    "yearly_df = pd.DataFrame({'Number requested' : numberofjobsatgivendcen}, index = yearly_index)\n",
    "\n",
    "ax = yearly_df.plot.bar(rot=0, title = 'Amount of servers requested per center total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "successes_2022_df = df_new2022.loc[df_new2022['Site Name'] == 'CTC-KC-III']\n",
    "successes_2022_df_values = successes_2022_df.value_counts(['First Site Name'])\n",
    "\n",
    "# Plotting the data to show the values\n",
    "successes_2022_df.value_counts(['First Site Name']).sort_values().plot(kind='barh', title='Amount of servers from a specific center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb800f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "63fb5e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other - VM | Other - VM', 'AX License Server | AXLIC - VM',\n",
       "       'AX Web Server | AXWS - VM', 'PVS Citrix | PVS Citrix - VM',\n",
       "       'PVS MGMT | PVS MGMT - VM', 'PVS Master | PVS Master - VM',\n",
       "       'Citrix Delivery Controller | XDC - VM',\n",
       "       'Citrix Session Recording | CSR - VM',\n",
       "       'PVS Gold Master CTX | PVS GM - VM',\n",
       "       'Olympus Guardian | OLYGD - VM',\n",
       "       'Domain Controller | Client DC - VM',\n",
       "       'Cerner Retail Pharmacy | CRPINT - VM', 'Kofax PDF | KFXPDF - VM',\n",
       "       'Citrix Storefront - External | SF (DMZ) - VM',\n",
       "       'Citrix Storefront - Internal | SF - VM', 'CSM APP > Non-Prod >VM',\n",
       "       'Resonance | Resonance', 'Domain Controller | Child DC - VM', nan,\n",
       "       'Websphere 8.5 LINUX - EAODR | EAODR - VM',\n",
       "       'Websphere 8.5 LINUX - EAWAS | EAWAS - VM',\n",
       "       'BE DB Tier->NonProd->DB Tier->Small - VM',\n",
       "       'PowerInsight Business Object | BO - VM',\n",
       "       'CareAware iBus | iBus - VM',\n",
       "       'CareAware Multi Media 7 | DICOM - Cerner DC - VM',\n",
       "       'Interqual | Interqual - VM',\n",
       "       'BE Application Tier | BE App Tier - VM', 'Gluster | Gluster - VM',\n",
       "       'OpenLink', 'Federal Content Catalog Web', 'Vitalslink | VL - VM',\n",
       "       'SQL - Always On | SQL - VM', 'ERMX | ERMX - VM',\n",
       "       'Federal Content Catalog Utility',\n",
       "       'Federal Content Catalog Database',\n",
       "       'Document Imaging Batch | BATCH-VM', '3M 360 Web | 3MEWEB - VM',\n",
       "       'OpenNMS | InfraOSS VM', 'Clairvia | CVWEB - VM',\n",
       "       'Clairvia | CVAPP - Configuration D - VM', 'Medanets DB',\n",
       "       'Clairvia | CVMID - VM', 'Clairvia | CVSQL - Configuration A - VM',\n",
       "       'Bridge Medical - SQL | BMSQL - VM',\n",
       "       'Clairvia | CVAPP - Configuration A - VM',\n",
       "       'Clairvia | CVMWEB - VM', 'Bridge Medical - WEB | BMWEB - VM',\n",
       "       'Bridge Medical - INT | BMINT - VM',\n",
       "       'Cerner Waveform Management | VM',\n",
       "       'Clairvia | CVINT - Configuration A - VM',\n",
       "       'Endoscopy Imaging | ENDO - VM', '3M Medicode', 'CERN_CCBH - VM',\n",
       "       'FetalLink | FetalLink Archive Server - VM',\n",
       "       'PVS Master MT | PVS Master MT - VM', 'OpenLink | OPNDB - VM',\n",
       "       'Millennium Crawler | Crawler - VM', 'SQL Server - VM |  SQL - VM',\n",
       "       'SFTP | SFTP - VM', 'Apache Outcome | Web/BO - VM',\n",
       "       'ACIS | ACIS - VM', 'RRD | RRD - VM',\n",
       "       'Citrix NON-PVS | Citrix Large - VM',\n",
       "       'Experian OPS | Experian OPS - VM',\n",
       "       'CareAware Multi Media 6 | CAMM VM - Cerner DC',\n",
       "       'HIE App | HIE App - VM', 'Clairvia | CVCTX - VM',\n",
       "       'Document Imaging | CPDI DX - VM',\n",
       "       'Clairvia | CVAPP - NP - Configuration AB - VM',\n",
       "       '3M CRS | 3M CRS 4CPU/12GB - VM',\n",
       "       'Citrix Gold Master | CSM - VM - Virtual',\n",
       "       'Citrix NON-PVS | Citrix Small - VM',\n",
       "       'CareTracker -> Prod/NonProd -> WEB | VM',\n",
       "       'CareTracker -> Prod/NonProd -> SVC | VM',\n",
       "       'CareTracker -> Prod/NonProd -> PUB | VM',\n",
       "       'Clairvia | CVINT - Configuration C - VM', 'Powerchart ECG | Virt',\n",
       "       '3M 360 Database | 3MEDB - VM', 'NFS | NFS - VM',\n",
       "       'P2Sentinel | P2S Single Node - VM', 'Medanets Web',\n",
       "       'UK Spine Websphere 8.5 EAWAS | VM',\n",
       "       'UK Spine Websphere 8.5 EAODR | VM',\n",
       "       'Document Imaging | ImageImport IMGIMP - VM',\n",
       "       'Charon | Charon - VM', 'OptumInsight (Ingenix) | OI - VM',\n",
       "       'P2Sentinel | P2S Analyzer/Collector - VM',\n",
       "       'P2Sentinel | P2S SLS - VM', 'P2Sentinel | P2S NonProd - VM',\n",
       "       '3M 360 Application | 3MEAPP - VM',\n",
       "       '3M CRS | 3M CRS 6CPU/24GB - VM',\n",
       "       'CERN_ERX - ePrescribing Services | VM',\n",
       "       '3M 360 Report | 3MERPT - VM', '3M 360 Interface | 3MEINT - VM',\n",
       "       'Ipswitch Progress SFTP SQL Server | CSM - VM - Virtual',\n",
       "       'Skybox Mobility', 'Infusion Mgmt | Infusion Mgmt SOAD - VM',\n",
       "       'CareAware Container Agent | VM',\n",
       "       'Cerner Retail Pharmacy | CRPCTX - VM', 'csm infra',\n",
       "       'OTTR - SQL | OTTRSQL - VM', 'OTTR - INT | OTTRINT - VM',\n",
       "       'Clairvia | CVINT - Configuration B - VM',\n",
       "       'Zabbix Proxy | ZBXP - VM', 'Dentrix | DTXINT - VM',\n",
       "       'Zabbix Server | ZBX', 'Advanced Capture Millennium',\n",
       "       'IDM SSPR | NetIQ SSPR - VM - Virtual',\n",
       "       'Clairvia->Nonprod->CVSQL - Configuration A',\n",
       "       'Citrix Store Front | CSM - VM - Virtual',\n",
       "       'Care Management - SQL | CMSQL - VM',\n",
       "       'Oracle Database Server | CSM - VM - Virtual',\n",
       "       'Cerner Retail Pharmacy | CRPSQL - VM',\n",
       "       'PowerChart ECG | EMG - VM',\n",
       "       'Clairvia | CVINT - Configuration D - VM',\n",
       "       'Clairvia | CVAPP - Configuration B - VM',\n",
       "       'PowerChart ECG | ECG - VM',\n",
       "       'Contract Management | Contract Management - VM', 'Webserver | VM',\n",
       "       'Document Management (DM)', 'Citrix PVS | CSM - VM - Virtual',\n",
       "       'Content 360 SQL | SQL - VM', 'OTTR - WEB | OTTRWEB - VM',\n",
       "       'Millennium+ Write Back Service | Millennium+ Write Back - VM',\n",
       "       'Citrix FAS', 'mTuitive Synoptic SQL | APSRSQL - VM',\n",
       "       'Millennium Mobile - ODR | MMODR - VM',\n",
       "       'Millennium Mobile - WAS | MMWAS - VM',\n",
       "       'Clairvia | CVAPP - Configuration C - VM',\n",
       "       'Cerner Retail Pharmacy Prod | Retail Pharmacy',\n",
       "       'Apache Outcome | CTP - VM', 'Federal Management Server | VM',\n",
       "       'RSA SecureID Authentication Manager->NONPROD->VM (for DARS)',\n",
       "       'Citrix Application Server | CSM - VM - Virtual',\n",
       "       'Citrix Delivery Controller | CSM - VM - Virtual',\n",
       "       'IDM eDirectory | IDM eDirectory - VM',\n",
       "       'Infoblox DNS Zone Forwarder',\n",
       "       'Django Patient Portal - Application | Django Patient Portal - VM',\n",
       "       '3M CRS | 3M CRS 2CPU/4GB - VM', 'Support Smart App',\n",
       "       'CloudSpaces - CERN_CLDSP', 'Beyond Now Citrix | BNTCTX - VM',\n",
       "       'CERN_C_SEC Tanium SQL Server | CSM - VM - Virtual',\n",
       "       'Citrix Database Server | CSM - VM - Virtual',\n",
       "       'IDM Management | NetIQ Mgmt - VM - Virtual',\n",
       "       'IDM RemoteLoader | IDM Remote Loader -VM -Virtual',\n",
       "       'Citrix DHCP | CSM - VM - Virtual',\n",
       "       'Remedy Corp Internal | Remedy Corp Internal - VM',\n",
       "       'SQL Server - VM | SQL - VM', 'Gajema | GAJ - VM',\n",
       "       'Millennium Search - Crawler | CRWL - VM',\n",
       "       'Remedy Corp External | Remedy Corp External - VM',\n",
       "       'Lighthouse - Business Objects | BO - VM',\n",
       "       'CERN_C_SEC | SEC - VM - Virtual',\n",
       "       'BE Database Tier | BE DB Tier - VM (deprecated)',\n",
       "       'Citrix Smart Auditor | CSA - VM',\n",
       "       'Websphere 8 - EAODR | EAODR - VM',\n",
       "       'Websphere 8 - EAWAS | EAWAS - VM',\n",
       "       'PVS Gold Master CTX - Temp | PVS GMT - VM',\n",
       "       'SAP-HANA | HANA - DC Berlin', 'API | AP Imaging - VM',\n",
       "       'ePrescribe Organization Directory | ePrescribe Organization Directory',\n",
       "       'Beyond Now Digital Media Archive | BNTDMA', 'Chart | CHRT - VM',\n",
       "       'Beyond Now HL7 | BNTHL7',\n",
       "       'Beyond Now Digital Media Archive | BNTDMA - VM',\n",
       "       'Beyond Now HL7 | BNTHL7 - VM', 'Beyond Now Citrix | BNTCTX',\n",
       "       '3M CRS | 3M CRS 6CPU/16GB - VM', 'Websphere | WAS - VM',\n",
       "       'Sybase | Sybase - VM', 'Web Interface | WI - VM',\n",
       "       'BE Database Tier->Non Prod->BE DB Tier',\n",
       "       'Beyond Now Automated Distribution | BNTAD - VM (good)',\n",
       "       'Beyond Now Automated Distribution | BNTAD - VM',\n",
       "       'Citrix Data Collector | Data Collector - VM',\n",
       "       'Peoplesoft - Application | PSFTAPP - VM',\n",
       "       'CareAware Connect SOAD | SOAD Texting - VM'], dtype=object)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Solution Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "658b1f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([133,  12,  13, 138, 141, 143,  58,  64, 140, 127,  94,  48, 114,\n",
       "        67,  68,  36, 153,  93, 177, 169, 170,  18, 147,  42,  41, 112,\n",
       "        17, 104, 128, 100, 164, 156,  95,  99,  98,  89,   4, 130,  82,\n",
       "        72, 116,  79,  81,  30,  69,  80,  31,  29,  50,  75,  96,   9,\n",
       "        32, 102, 142, 129, 118, 157, 155,  16,  10, 149,  61,  97,  40,\n",
       "       105,  74,  90,  73,   6,  60,  62,  45,  44,  43,  77, 148,   1,\n",
       "       123, 137, 117, 163, 162,  91,  51, 131, 134, 136, 135,   0,   8,\n",
       "        35,   3,   2, 113, 159, 111,  39,  47, 174, 125, 124,  76, 172,\n",
       "        87, 173,  14, 108,  83,  66,  37, 132,  49, 146,  78,  70, 145,\n",
       "        86, 166,  92,  63,  85, 126, 122,  59, 176, 119, 120,  71,  46,\n",
       "        15, 101, 150,  53,  57, 109, 110,  88,   5, 160,  84,  24,  33,\n",
       "        56, 106, 107,  54, 152, 158, 103, 121, 151, 115,  34,  19,  65,\n",
       "       167, 168, 139, 154,  11, 175,  25,  52,  27,  26,  28,  23,   7,\n",
       "       171, 161, 165,  20,  22,  21,  55, 144,  38])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Solution Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e107135e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'db1c6643-54fd-4d71-ad9c-9f5bd6baf857',\n",
       "       'b32bd8cb-9709-47ca-93f2-4eab4c26546f',\n",
       "       'a5bd8b00-e21c-47fa-826c-8aea3ac9f9ea',\n",
       "       'a3920a41-5eba-4635-a535-82462555337e',\n",
       "       'eaf716fd-5861-43a9-9b48-d0f00466d723',\n",
       "       'f1201d70-6cb8-46da-ab7c-0da1204126d7',\n",
       "       '3d80e882-9f27-4211-a7c5-9ef938661869',\n",
       "       '0768150d-1804-4da0-9f45-406be1909ba5',\n",
       "       '71f158a6-76b6-472f-8447-3574c09722f6',\n",
       "       'dfeb48ef-7579-475a-a3bd-fc34f62cfec8',\n",
       "       'd4da5d55-4f1a-4b16-9bcd-21c5a06d97c2',\n",
       "       '4170ceb0-8ddd-46fa-ab0c-a9818820e61f',\n",
       "       '53801ead-8242-437f-9f8f-e01f14a25217',\n",
       "       '6f76a25e-887e-404b-b08c-7de9b12c803f',\n",
       "       '3aa2eb00-320c-4b75-8a41-04e443fdb47b',\n",
       "       'c29244c6-135f-49d6-97d6-3706a629533b',\n",
       "       'ddb84ef6-7dda-4e59-9bcb-40c1e75c98e3',\n",
       "       'f260d454-756c-49f0-9430-eba1d03bca26',\n",
       "       '67487f1c-43da-4cf5-8831-9ef80abeee55',\n",
       "       '5056aa76-930d-4756-808c-1b76dafebbf6',\n",
       "       '77405803-5478-41bf-b783-35c4d4ae7e8a',\n",
       "       '76051ca5-c21c-4000-be66-b077d0819088',\n",
       "       'a0160ef0-d654-4b0d-bb83-1b42a987b41a',\n",
       "       '5453c9ca-7f0f-43c8-b615-c579451efc1c',\n",
       "       'b9047b69-881c-4e8d-be66-6987b54a9822',\n",
       "       '619d22bb-80f0-423b-aba6-60c7a0ab41da',\n",
       "       'ba932e53-2940-4fc0-beb5-e87fb737459a',\n",
       "       '2a94dc44-2697-4603-a368-e1c0d10fa711'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Hardware Site ID'].unique()\n",
    "#unique server names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
