{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UUW_HgvyV2ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2050d4a2-bbc3-4502-b260-0ffbb348f02a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "s-z9BAS-ULo8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JBlpWA-5ULpE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/oracle/ClosedStack_data_Virtual.csv', low_memory= False, encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KiRNWCIBULpF"
      },
      "outputs": [],
      "source": [
        "df_new = df[[ 'Job ID','Hardware ID', 'Hardware Name', 'Solution ID', 'Ticket Site','Site Name', 'Job Created']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JrC7tMDLHTLb"
      },
      "outputs": [],
      "source": [
        "df_new_3 = df_new.loc[(df_new['Job Created'] <= \"2022-09-01 00:00:00.000000\") & (df_new['Job Created'] >= \"2020-08-02 00:00:00.000000\")]\n",
        "df_filtered1 = df_new_3.dropna(subset=['Job ID', 'Job Created', 'Solution ID']).copy()\n",
        "df_filtered1.loc[:, 'Job Created'] = pd.to_datetime(df_filtered1['Job Created']).dt.strftime('%Y-%m-%d')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ApkDURS-HXzr"
      },
      "outputs": [],
      "source": [
        "# One-Hot encode the 'Solution ID' column\n",
        "onehot_encoder = OneHotEncoder()\n",
        "solution_id_encoded = onehot_encoder.fit_transform(df_filtered1[['Solution ID']]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot encode the categorical columns (in this case, 'Job ID')\n",
        "categorical_cols = ['Job ID']\n",
        "encoder = OneHotEncoder()\n",
        "categorical_encoded = encoder.fit_transform(df_filtered1[categorical_cols]).toarray()"
      ],
      "metadata": {
        "id": "2YtjsM3MHaJk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoder = OneHotEncoder()\n",
        "jobcreated_encoded = onehot_encoder.fit_transform(df_filtered1[['Job Created']]).toarray()"
      ],
      "metadata": {
        "id": "KuLdg9glHhIs"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate one-hot encoded 'Solution ID' and categorical columns\n",
        "X_categorical = np.concatenate([categorical_encoded, solution_id_encoded], axis=1)"
      ],
      "metadata": {
        "id": "P5imDHM2HiRy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([jobcreated_encoded], axis=1)"
      ],
      "metadata": {
        "id": "VYKgIsk7HlQK"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA for combined features\n",
        "pca_categorical = PCA(n_components=50)\n",
        "X_catagorical_pca = pca_categorical.fit_transform(X_categorical)"
      ],
      "metadata": {
        "id": "5qfdRi5PHnZZ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_catagorical_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train2 = X_train2.astype(np.float64)\n",
        "X_test2 = X_test2.astype(np.float64)\n",
        "\n",
        "y_train2 = y_train2.astype(np.float64)\n",
        "y_test2 = y_test2.astype(np.float64)"
      ],
      "metadata": {
        "id": "isr2CeMKHpPt"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train2, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train2, dtype=torch.float32)\n",
        "if y_train_tensor.dim() == 3:\n",
        "    y_train_tensor = y_train_tensor.squeeze(2)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test2, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test2, dtype=torch.float32)\n",
        "if y_test_tensor.dim() == 3:\n",
        "    y_test_tensor = y_test_tensor.squeeze(2)"
      ],
      "metadata": {
        "id": "FHffOMWhHtBR"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for training and testing data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "1r7XPwMKHvrh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Input size is the number of features after PCA\n",
        "        self.fc2 = nn.Linear(128, 581)  # Output size is 1 for regression task\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KgtjE1iHHywb"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork(input_size=X_catagorical_pca.shape[1])\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "XgLQa7zBH1LJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "qWvtoOrZH4bw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    mse = criterion(test_outputs, y_test_tensor)\n",
        "    print(\"Mean Squared Error on Test Data:\", mse.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-m7oD6lH7bA",
        "outputId": "fbf36b3e-878b-4e6b-aafb-997631a8f7b6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Data: 0.0016873020213097334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "22Tigd6_ULpG"
      },
      "outputs": [],
      "source": [
        "df_new_2 = df_new.loc[(df_new['Job Created'] <= \"2020-08-01 00:00:00.000000\") & (df_new['Job Created'] >= \"2019-01-01 00:00:00.000000\")]\n",
        "df_filtered = df_new_2.dropna(subset=['Job ID', 'Job Created', 'Solution ID']).copy()\n",
        "df_filtered.loc[:, 'Job Created'] = pd.to_datetime(df_filtered['Job Created']).dt.strftime('%Y-%m-%d')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "acOzqMFmULpI"
      },
      "outputs": [],
      "source": [
        "# One-Hot encode the 'Solution ID' column\n",
        "onehot_encoder = OneHotEncoder()\n",
        "solution_id_encoded = onehot_encoder.fit_transform(df_filtered[['Solution ID']]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "pUlUKVQ8ULpJ"
      },
      "outputs": [],
      "source": [
        "# One-Hot encode the categorical columns (in this case, 'Job ID')\n",
        "categorical_cols = ['Job ID']\n",
        "encoder = OneHotEncoder()\n",
        "categorical_encoded = encoder.fit_transform(df_filtered[categorical_cols]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WVUsMvsRULpJ"
      },
      "outputs": [],
      "source": [
        "onehot_encoder = OneHotEncoder()\n",
        "jobcreated_encoded = onehot_encoder.fit_transform(df_filtered[['Job Created']]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "onB4TkUgULpK"
      },
      "outputs": [],
      "source": [
        "# Concatenate one-hot encoded 'Solution ID' and categorical columns\n",
        "X_categorical = np.concatenate([categorical_encoded, solution_id_encoded], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "7UWxMBCPULpL"
      },
      "outputs": [],
      "source": [
        "y = np.concatenate([jobcreated_encoded], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "F8JyhIUbULpM"
      },
      "outputs": [],
      "source": [
        "# PCA for combined features\n",
        "pca_categorical = PCA(n_components=50)\n",
        "X_catagorical_pca = pca_categorical.fit_transform(X_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uVY3qWqTULpN"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_catagorical_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train1 = X_train1.astype(np.float64)\n",
        "X_test1 = X_test1.astype(np.float64)\n",
        "\n",
        "y_train1 = y_train1.astype(np.float64)\n",
        "y_test1 = y_test1.astype(np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DrIN48plULpO"
      },
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train1, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train1, dtype=torch.float32)\n",
        "if y_train_tensor.dim() == 3:\n",
        "    y_train_tensor = y_train_tensor.squeeze(2)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test1, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test1, dtype=torch.float32)\n",
        "if y_test_tensor.dim() == 3:\n",
        "    y_test_tensor = y_test_tensor.squeeze(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Dk0HVCryULpO"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader for training and testing data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xMxqbjjcULpP"
      },
      "outputs": [],
      "source": [
        "# Define custom neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Input size is the number of features after PCA\n",
        "        self.fc2 = nn.Linear(128, 409)  # Output size is 1 for regression task\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "tHkyzOISULpP"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork(input_size=X_catagorical_pca.shape[1])\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hTlTryA_ULpP"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "dyWdV78ZULpQ",
        "outputId": "40e853be-6506-4146-a121-8668346b5d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Data: 0.0024019854608923197\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    mse = criterion(test_outputs, y_test_tensor)\n",
        "    print(\"Mean Squared Error on Test Data:\", mse.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_X_train = np.concatenate([X_train1, X_train2], axis=0)\n",
        "combined_y_train = np.concatenate([y_train1, y_train2], axis=0)\n",
        "\n",
        "combined_X_test = np.concatenate([X_test1, X_test2], axis=0)\n",
        "combined_y_test = np.concatenate([y_test1, y_test2], axis=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Omp_NPiPQvJd",
        "outputId": "f7c9ffd2-f72c-4a01-da5d-a89c615086cf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-e2eea09a8f57>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcombined_X_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcombined_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcombined_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 409 and the array at index 1 has size 581"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q168h-zULpQ"
      },
      "outputs": [],
      "source": [
        "df['Solution ID'].unique()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}